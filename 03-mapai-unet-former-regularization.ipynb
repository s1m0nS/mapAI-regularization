{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "1KJnP7zSGQg5"
      },
      "source": [
        "## Binary semantic segmentation example using U-Net-Former\n",
        "Preparation of dataset and model training code from here:\n",
        "\n",
        "https://pyimagesearch.com/2021/11/08/u-net-training-image-segmentation-models-in-pytorch/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFxHJWmXlcZk",
        "outputId": "b755d1a8-3650-42d9-8718-401b4458f049"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchvision.__version__)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)\n",
        "\n",
        "# determine if we will be pinning memory during data loading\n",
        "PIN_MEMORY = True if DEVICE == \"cuda\" else False"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KVfaGZrWG63Q"
      },
      "source": [
        "#### CONFIGURE PATHS AND HYPERPARAMETERS FOR TRAINING BELOW."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "OjlBC-raVM2K",
        "outputId": "b6950142-9c43-40bd-9edb-e10b3973f745"
      },
      "outputs": [],
      "source": [
        "GD_PATH = os.getcwd() # get current working directory for the repo\n",
        "print(GD_PATH)\n",
        "\n",
        "# PROVIDE PATH TO DOWNLOADED MAPAI DATASET\n",
        "DATASET_PATH = \"/home/shymon/datasets/\"\n",
        "\n",
        "DATASET_PATH = os.path.join(DATASET_PATH, \"mapai_full\") # create dataset path\n",
        "\n",
        "print(DATASET_PATH)\n",
        "\n",
        "TRAIN_IMG_DIR = os.path.join(DATASET_PATH, \"train\", \"images\")\n",
        "TRAIN_MASK_DIR = os.path.join(DATASET_PATH, \"train\", \"masks\")\n",
        "\n",
        "print(TRAIN_IMG_DIR)\n",
        "print(TRAIN_MASK_DIR)\n",
        "\n",
        "VAL_IMG_DIR = os.path.join(DATASET_PATH, \"validation\", \"images\")\n",
        "VAL_MASK_DIR = os.path.join(DATASET_PATH, \"validation\", \"masks\")\n",
        "\n",
        "print(VAL_IMG_DIR)\n",
        "print(VAL_MASK_DIR)\n",
        "\n",
        "TEST_IMG_DIR = os.path.join(DATASET_PATH, \"task1_test\", \"images\")\n",
        "TEST_MASK_DIR = os.path.join(DATASET_PATH, \"task1_test\", \"masks\")\n",
        "\n",
        "print(TEST_IMG_DIR)\n",
        "print(TEST_MASK_DIR)\n",
        "\n",
        "# CONFIGURE MapAI DATASET\n",
        "NUM_CHANNELS = 3\n",
        "NUM_LEVELS  = 3\n",
        "NUM_CLASSES = 1\n",
        "\n",
        "# IMAGE SHAPE\n",
        "IMG_WIDTH = 512\n",
        "IMG_HEIGHT = 512\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------#\n",
        "\n",
        "# CONFIGURE parameters for training\n",
        "EPOCHS = 25\n",
        "init_lr = 1e-4 # learning rate\n",
        "BATCH_SIZE = 2\n",
        "\n",
        "THRESHOLD  = 0.5\n",
        "base_output = \"out\"\n",
        "\n",
        "model_name = \"unet-former-25-epochs.pth\" # provide name for model\n",
        "training_plot_name = \"unet-former-25-epochs.png\"\n",
        "\n",
        "#---------------------------------------------------------------------------------------------------#\n",
        "\n",
        "# OUTPUT PATHS\n",
        "\n",
        "# Trained model path\n",
        "MODEL_PATH = os.path.join(GD_PATH, \"trained_models\", model_name) # change depending on the number of epochs\n",
        "print(MODEL_PATH)\n",
        "PLOT_PATH  = os.path.join(GD_PATH, \"plots\", training_plot_name) # the folder to save future plots\n",
        "print(PLOT_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfSMUZbWWdJn"
      },
      "source": [
        "### Load and read the MapAI dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPiACQ_6VyQP"
      },
      "outputs": [],
      "source": [
        "import tifffile\n",
        "from torch.utils.data import Dataset\n",
        "import cv2\n",
        "\n",
        "\n",
        "class mapAIdataset(Dataset):\n",
        "    def __init__(self, imagePaths, maskPaths, transforms):\n",
        "        # store the image and mask filepaths, and augmentation\n",
        "        # transforms\n",
        "        self.imagePaths = imagePaths\n",
        "        self.maskPaths = maskPaths\n",
        "        self.transforms = transforms\n",
        "        \n",
        "    def __len__(self):\n",
        "        # return the number of total samples contained in the dataset\n",
        "        return len(self.imagePaths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # grab the image path from the current index\n",
        "        imagePath = self.imagePaths[idx]\n",
        "        # load the image from disk, swap its channels from BGR to RGB,\n",
        "        # and read the associated mask from disk\n",
        "        image = cv2.imread(imagePath)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        mask = tifffile.imread(self.maskPaths[idx])\n",
        "        # convert the mask to a float32 tensor with values in the range [0, 1]\n",
        "        mask = mask.astype('float32')\n",
        "        # check to see if we are applying any transformations\n",
        "        if self.transforms is not None:\n",
        "            # apply the transformations to both image and its mask\n",
        "            image = self.transforms(image)\n",
        "            mask = self.transforms(mask)\n",
        "    \n",
        "        # return a tuple of the image and its mask\n",
        "        return (image, mask)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "AKXL9bO8WnNg"
      },
      "source": [
        "### U-Net Former architecture\n",
        "\n",
        "Downloaded from: https://github.com/WangLibo1995/GeoSeg/blob/main/geoseg/models/UNetFormer.py\n",
        "saved into UNetFormer.py file, from where we import the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9urE3W1iWp7v"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "subfolder = os.path.join(GD_PATH, \"models\")\n",
        "sys.path.insert(0, subfolder)\n",
        "\n",
        "import UNetFormer_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22hbANvfWxmX"
      },
      "source": [
        "### Training the segmentation model\n",
        "Below we append the paths for TRAIN/VAL/TEST sets - images/masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2Jha-LCW0ir",
        "outputId": "fcad4c67-0851-42e6-ea68-6b6dd88d26c2"
      },
      "outputs": [],
      "source": [
        "from torch.nn import BCEWithLogitsLoss\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "from imutils import paths\n",
        "import time\n",
        "\n",
        "# TRAINING\n",
        "train_images = sorted(list(paths.list_images(TRAIN_IMG_DIR)))\n",
        "train_masks = sorted(list(paths.list_images(TRAIN_MASK_DIR)))\n",
        "\n",
        "# VALIDATION\n",
        "val_images = sorted(list(paths.list_images(VAL_IMG_DIR)))\n",
        "val_masks = sorted(list(paths.list_images(VAL_MASK_DIR)))\n",
        "\n",
        "\n",
        "# TEST\n",
        "test_images = sorted(list(paths.list_images(TEST_IMG_DIR)))\n",
        "test_masks = sorted(list(paths.list_images(TEST_MASK_DIR)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtqUNGR1XCa5"
      },
      "source": [
        "### Define transformations\n",
        "\n",
        "I tried out different data augmentation techniques, including Horizontal Flip, Vertical Flip, Contrast, Brightness. They did not improve my results much, the validation and training loss were actually worse than without data augmentation techniques."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ghW7Nj0OEQMc"
      },
      "source": [
        "https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py\n",
        "\n",
        "https://albumentations.ai/docs/getting_started/mask_augmentation/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR_dzdpCXCHY",
        "outputId": "4e9b1681-2846-489f-edf6-a6240af65563"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "# T.RandomHorizontalFlip(p=0.5),\n",
        "# T.RandomVerticalFlip(p=0.1),\n",
        "\n",
        "# Image augmentations applied\n",
        "transforms = T.Compose([T.ToPILImage(),\n",
        "                        T.Resize((IMG_HEIGHT,IMG_WIDTH)),\n",
        "                        T.ToTensor()])\n",
        "\n",
        "# create the train and test datasets\n",
        "trainDS = mapAIdataset(imagePaths=train_images,\n",
        "                       maskPaths=train_masks,\n",
        "                       transforms=transforms)\n",
        "\n",
        "valDS = mapAIdataset(imagePaths=val_images,\n",
        "                     maskPaths=val_masks,\n",
        "                     transforms=transforms)\n",
        "\n",
        "testDS = mapAIdataset(imagePaths=test_images,\n",
        "                      maskPaths=test_masks,\n",
        "                      transforms=transforms)\n",
        "\n",
        "print(f\"[INFO] found {len(trainDS)} examples in the TRAINING set...\")\n",
        "print(f\"[INFO] found {len(valDS)} examples in the VALIDATION set...\")\n",
        "print(f\"[INFO] found {len(testDS)} examples in the TEST set...\")\n",
        "\n",
        "# create the training and test data loaders\n",
        "trainLoader = DataLoader(trainDS,\n",
        "                         shuffle=True,\n",
        "                         batch_size=BATCH_SIZE,\n",
        "                         pin_memory=PIN_MEMORY,\n",
        "                         num_workers=os.cpu_count())\n",
        "\n",
        "valLoader = DataLoader(valDS,\n",
        "                       shuffle=False,\n",
        "                       batch_size=BATCH_SIZE,\n",
        "                       pin_memory=PIN_MEMORY,\n",
        "                       num_workers=os.cpu_count())\n",
        "\n",
        "testLoader = DataLoader(testDS,\n",
        "                        shuffle=False,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        pin_memory=PIN_MEMORY,\n",
        "                        num_workers=os.cpu_count())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "tAO9M_R4XG6q"
      },
      "source": [
        "### Initialize UNET-FORMER model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IMsYzUaXJW7",
        "outputId": "5328f1e2-4a71-4c05-fa9e-23b768971315"
      },
      "outputs": [],
      "source": [
        "model = UNetFormer_model.UNetFormer().to(DEVICE)\n",
        "\n",
        "# loss / optimizer\n",
        "lossFunction = BCEWithLogitsLoss()\n",
        "opt = Adam(model.parameters(), lr=init_lr, weight_decay=0.001)\n",
        "\n",
        "# calculate steps per epoch for train/val/test\n",
        "trainSteps = len(trainDS) // BATCH_SIZE \n",
        "valSteps = len(valDS) // BATCH_SIZE\n",
        "testSteps = len(testDS) // BATCH_SIZE\n",
        "\n",
        "print(trainSteps, valSteps, testSteps)\n",
        "\n",
        "# initialize a dictionary to store training history\n",
        "H = {\"train_loss\": [], \"val_loss\": [], \"test_loss\": []}\n",
        "H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEP-IVokbWQg"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xcjuKhMeXLU-"
      },
      "source": [
        "### TRAINING THE MODEL\n",
        "\n",
        "Run this piece of code only if you want to train the model from scratch.\n",
        "\n",
        "Training locally: BATCH_SIZE  = 2 takes 5035 MB of GPU memory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWuUyLUgXPNf",
        "outputId": "34c6485b-838b-45cf-99da-601f7cfbc4d1"
      },
      "outputs": [],
      "source": [
        "# loop over epochs\n",
        "print(\"[INFO] training UNET-FORMER ...\")\n",
        "startTime = time.time()\n",
        "\n",
        "for epoch in tqdm(range(EPOCHS)):\n",
        "    model.train()\n",
        "\n",
        "    # initialize total training and validation loss\n",
        "    totalTrainLoss = 0\n",
        "    totalValLoss = 0\n",
        "    totalTrainAcc = 0\n",
        "    totalValAcc = 0\n",
        "\n",
        "    # loop over the training set\n",
        "    for (i, (x, y)) in enumerate(trainLoader):\n",
        "        # send output to device\n",
        "        (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "\n",
        "        # perform a forward pass and calculate the training loss\n",
        "        pred = model(x)\n",
        "        loss = lossFunction(pred, y)\n",
        "\n",
        "        # calculate the accuracy\n",
        "        acc = ((pred > 0.5) == y).float().mean()\n",
        "\n",
        "        # kill previously accumulated gradients then\n",
        "        # perform backpropagation and update model parameters\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # add the loss and accuracy to the total training loss and accuracy\n",
        "        totalTrainLoss += loss\n",
        "        totalTrainAcc += acc\n",
        "\n",
        "    # switch of autograd\n",
        "    with torch.no_grad():\n",
        "        # set the model in evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # loop over the validation set\n",
        "        for (x, y) in valLoader:\n",
        "            # send the input to the device\n",
        "            (x, y) = (x.to(DEVICE), y.to(DEVICE))\n",
        "\n",
        "            # make the predictions and calculate the validation loss\n",
        "            pred = model(x)\n",
        "            loss = lossFunction(pred, y)\n",
        "\n",
        "            # calculate the accuracy\n",
        "            acc = ((pred > 0.5) == y).float().mean()\n",
        "\n",
        "            # add the loss and accuracy to the total validation loss and accuracy\n",
        "            totalValLoss += loss\n",
        "            totalValAcc += acc\n",
        "\n",
        "    # calculate the average training and validation loss and accuracy\n",
        "    avgTrainLoss = totalTrainLoss / trainSteps\n",
        "    avgValLoss = totalValLoss / valSteps\n",
        "    avgTrainAcc = totalTrainAcc / trainSteps\n",
        "    avgValAcc = totalValAcc / valSteps\n",
        "        \n",
        "    # update our training history\n",
        "    H[\"train_loss\"].append(avgTrainLoss.cpu().detach().numpy())\n",
        "    H[\"val_loss\"].append(avgValLoss.cpu().detach().numpy())\n",
        "\n",
        "    # print the model training and validation information\n",
        "    print(\"[INFO] EPOCH: {}/{}\".format(epoch + 1, EPOCHS))\n",
        "    print(\"Train loss: {:.6f}, Train acc: {:.6f}, Val loss: {:.4f}, Val acc: {:.4f}\".format(\n",
        "        avgTrainLoss, avgTrainAcc, avgValLoss, avgValAcc))\n",
        "        \n",
        "# display the total time needed to perform the training\n",
        "endTime = time.time()\n",
        "print(\"[INFO] total time taken to train the model: {:.2f}s\".format(endTime - startTime))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsJoOVn11rs9",
        "outputId": "c9a36460-f773-4771-cfd7-fec78711d8cc"
      },
      "outputs": [],
      "source": [
        "H # show traning/val loss history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6ChLXHuXZHA"
      },
      "source": [
        "### Plot the training and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "j04HfubrXYvX",
        "outputId": "5416f4a7-2647-40e9-ce25-cac7909dea50"
      },
      "outputs": [],
      "source": [
        "# plot the training loss\n",
        "print(MODEL_PATH)\n",
        "print(PLOT_PATH)\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(H[\"train_loss\"], label=\"train_loss\")\n",
        "plt.plot(H[\"val_loss\"], label=\"val_loss\")\n",
        "plt.title(\"Training Loss on Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(PLOT_PATH)\n",
        "# serialize the model to disk\n",
        "torch.save(model, MODEL_PATH) # saves the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Y6Fx2oaWr0q"
      },
      "source": [
        "### Prediction part\n",
        "\n",
        "Here the trained model is loaded and use for prediction on test images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYh4flMu7O-m",
        "outputId": "94909220-5b1f-43ad-b52f-d37bbaa270fd"
      },
      "outputs": [],
      "source": [
        "# Load saved model for prediction\n",
        "\n",
        "print(MODEL_PATH)\n",
        "\n",
        "model = torch.load(MODEL_PATH) # add MODEL_PATH after training\n",
        "print(\"model loaded for prediction\")\n",
        "\n",
        "model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Provide test images for MapAI Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PREDICTIONS_DIR = os.path.join(GD_PATH, \"predictions\")\n",
        "PREDICTIONS_DIR"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Make predictions on the entire MapAI dataset\n",
        "\n",
        "Make predictions on test images and save them to the folder named predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bq7BlbdrcgPB",
        "outputId": "6860afd9-da51-4911-d975-5a3ed78e01e1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import gc\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# PLOTTING PREDICTIONS AS SINGLE IMAGES\n",
        "\n",
        "# Output folder for the predictions\n",
        "output_folder = PREDICTIONS_DIR + \"/\" # check for Windows to save predictions inside the folder\n",
        "\n",
        "# PLOT TEST IMAGES as RGB\n",
        "for n in range(len(test_images)):\n",
        "  gc.collect()\n",
        "  # Test image number\n",
        "  testImgName = str(Path(test_images[n]).stem) + '.tif'\n",
        "  #print('#', testImgName)\n",
        "\n",
        "   # Make predicton on a test image specified with counter n\n",
        "  test_img = test_images[n]\n",
        "  test_img_input = np.expand_dims(test_img, 0)\n",
        "  #print('#', test_img_input[0])\n",
        "\n",
        "  # PyTorch --> works\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    image = cv2.imread(test_img_input[0])\n",
        "    image = cv2.resize(image, dsize = (IMG_WIDTH, IMG_HEIGHT), interpolation=cv2.INTER_CUBIC)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    image = image.astype(\"float32\") / 255\n",
        "    \n",
        "    # print('SIZE: ', image.shape)\n",
        "\n",
        "    # make the channel axis to be the leading one, add batch dimension\n",
        "    image = np.transpose(image, (2, 0, 1))\n",
        "    # create a PyTorch tensor\n",
        "    image = np.expand_dims(image, 0)\n",
        "    # flash the tensor to the device\n",
        "    image  = torch.from_numpy(image).to(DEVICE)\n",
        "\n",
        "    # make the prediction\n",
        "    predMask = model(image).squeeze()\n",
        "    # pass result through sigmoid\n",
        "    predMask = torch.sigmoid(predMask)\n",
        "\n",
        "    # convert result to numpy array\n",
        "    predMask = predMask.cpu().numpy()\n",
        "\n",
        "    # filter out the weak predictions and convert them to integers\n",
        "    predMask = (predMask > THRESHOLD) * 255\n",
        "    predMask = predMask.astype(np.uint8)\n",
        "\n",
        "    # generate image from array\n",
        "    pIMG = Image.fromarray(predMask)\n",
        "    pIMG.save(str(output_folder + testImgName))\n",
        "\n",
        "    print('Prediction:', testImgName, 'saved to:', output_folder)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Make predictions on single images by choice\n",
        "\n",
        "Change the parameter n to choose which image to plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ----------------------------------------------------------------------\n",
        "\n",
        "predictions = glob.glob(output_folder + \"*.tif\")\n",
        "predictions.sort()\n",
        "print(\"# IMAGES for prediction: \", len(predictions))\n",
        "print(\"Choosen n can be from 0 o 1367! \")\n",
        "\n",
        "# ----------------------------------------------------------------------\n",
        "\n",
        "n = 900 # change this number depending on which image you want to test\n",
        "\n",
        "fig = plt.figure(figsize=(18,12))\n",
        "ax1 = fig.add_subplot(131)\n",
        "\n",
        "ax1.set_title('RGB image: ')\n",
        "image = cv2.imread(test_images[n])[:,:,::-1]\n",
        "ax1.imshow(image)\n",
        "ax1.set_axis_off()\n",
        "\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.set_title('Ground truth: ')\n",
        "image = cv2.imread(test_masks[n])[:,:,::-1]\n",
        "image *= 255\n",
        "ax2.imshow(image)\n",
        "ax2.set_axis_off()\n",
        "\n",
        "ax3 = fig.add_subplot(133)\n",
        "ax3.set_title('Prediction: ')\n",
        "image = cv2.imread(predictions[n])[:,:,::-1]\n",
        "ax3.imshow(image)\n",
        "ax3.set_axis_off()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg_0qxbcjzfw"
      },
      "source": [
        "### BUILDING FOOTPRINT REGULARIZATION\n",
        "\n",
        "Used repo: https://github.com/zorzi-s/projectRegularization\n",
        "\n",
        "git clone the repo to the folder where your notebook is stored. To get curent working directory use os.getcwd().\n",
        "\n",
        "The pretrained weights need to be downloaded from the provided link and saved into the folder pretrained_weighs that is inside projectRegularization:\n",
        "\n",
        "https://drive.google.com/drive/folders/1IPrDpvFq9ODW7UtPAJR_T-gGzxDat_uu"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next step is to generate a Python file to locate the necessary pretrained weights from projectRegularization. The code below was only tested on Ubuntu, not on Windows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yp8uKrNUjyGn"
      },
      "outputs": [],
      "source": [
        "# DEFINE NECESSARY PATHS FOR REGULARIZATION PART\n",
        "\n",
        "projectRegDir = os.path.join(GD_PATH, \"projectRegularization\")\n",
        "print(projectRegDir)\n",
        "\n",
        "ptw = os.path.join(projectRegDir, \"pretrained_weights\") \n",
        "print(ptw)\n",
        "\n",
        "# OUTPUT REGULARIZATIONS DIR\n",
        "REGULARIZATION_DIR = os.path.join(GD_PATH, \"regularizations\") + \"/\"\n",
        "print(REGULARIZATION_DIR)\n",
        "\n",
        "# GET THE PATHS FOR TRAINED GAN MODELS\n",
        "ENCODER = os.path.join(ptw, \"E140000_e1\")\n",
        "GENERATOR = os.path.join(ptw, \"E140000_net\")\n",
        "\n",
        "print(ENCODER)\n",
        "print(GENERATOR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CREATE A NEW variables.py WITH USERS PATHS\n",
        "\n",
        "with open(projectRegDir + 'variables.py', 'w') as f:\n",
        "    f.write('# CONFIGURE THE PATHS HERE: \\n\\n')\n",
        "    f.write('# TRAINING \\n')\n",
        "    f.write('DATASET_RGB = ' + '\"' + str(TRAIN_IMG_DIR + '*.tif' + '\"') + '\\n')\n",
        "    f.write('DATASET_GTI = ' + '\"' + str(TRAIN_MASK_DIR + '*.tif' + '\"') + '\\n')\n",
        "    f.write('DATASET_SEG = ' + '\"' + str(PREDICTIONS_DIR + '*.tif' + '\"') + '\\n')\n",
        "    f.write('\\n')\n",
        "    f.write('DEBUG_DIR = ' + '\"' + str('./debug/') + '\"' + '\\n')\n",
        "    f.write('\\n')\n",
        "    f.write('# INFERENCE \\n')\n",
        "    f.write('INF_RGB = ' + '\"' + str(TEST_IMG_DIR + '*.tif' + '\"') + '\\n')\n",
        "    f.write('INF_SEG = ' + '\"' + str(PREDICTIONS_DIR + '*.tif' + '\"') + '\\n')\n",
        "    f.write('INF_OUT = ' + '\"' + str(REGULARIZATION_DIR + '\"') + '\\n')\n",
        "    f.write('\\n')\n",
        "    f.write('MODEL_ENCODER = ' + '\"' + str(ENCODER) + '\"' + '\\n')\n",
        "    f.write('MODEL_GENERATOR = ' + '\"' + str(GENERATOR) + '\"' + '\\n')\n",
        "    f.close()\n",
        " \n",
        "print(\"variables.py created with users paths...\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Run projectRegularization\n",
        "\n",
        "Takes around 6-8 minutes.\n",
        "\n",
        "You only need to change the command below and replace it with the absolute path for regularize.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python /home/shymon/Documents/mapAI-regularization/projectRegularization/regularize.py"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare predictions and regularizations on a single image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89nW6Q7F6aga",
        "outputId": "976d62a2-76a7-4b52-a4bc-218f63d8a122"
      },
      "outputs": [],
      "source": [
        "# Read Regularizations to plot and compare results\n",
        "\n",
        "regularizations = glob.glob(REGULARIZATION_DIR + \"*.tif\")\n",
        "regularizations.sort()\n",
        "\n",
        "print(\"# of predicted images: \", len(predictions))\n",
        "print(\"# of regularized images: \", len(regularizations))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Code to plot RGB, GT, PREDICTION and REGULARIZATION in a single plot for comparison.\n",
        "\n",
        "Change parameter n accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 600\n",
        "\n",
        "fig = plt.figure(figsize=(18,12))\n",
        "ax1 = fig.add_subplot(141)\n",
        "\n",
        "ax1.set_title('RGB: ')\n",
        "image = cv2.imread(test_images[n])[:,:,::-1]\n",
        "ax1.imshow(image)\n",
        "ax1.set_axis_off()\n",
        "\n",
        "ax2 = fig.add_subplot(142)\n",
        "ax2.set_title('Ground truth: ')\n",
        "image = cv2.imread(test_masks[n])[:,:,::-1]\n",
        "image *= 255\n",
        "ax2.imshow(image)\n",
        "ax2.set_axis_off()\n",
        "\n",
        "ax3 = fig.add_subplot(143)\n",
        "ax3.set_title('Prediction: ')\n",
        "image = cv2.imread(predictions[n])[:,:,::-1]\n",
        "ax3.imshow(image)\n",
        "ax3.set_axis_off()\n",
        "\n",
        "ax4 = fig.add_subplot(144)\n",
        "ax4.set_title('Regularization: ')\n",
        "image = cv2.imread(regularizations[n])[:,:,::-1]\n",
        "ax4.imshow(image)\n",
        "ax4.set_axis_off()\n",
        "\n",
        "# DEFINE PATH FOR PLOTS TO BE SAVED\n",
        "figPath = GD_PATH + \"/\" + \"plots\" + \"/\" \"compare-\" + str(n) + \".png\"\n",
        "print(figPath)\n",
        "\n",
        "# Save plot\n",
        "fig.savefig(figPath)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VECTORIZING THE REGULARIZED BUILDING MASKS with GDAL\n",
        "\n",
        "GDAL: https://gdal.org/'\n",
        "\n",
        "GDAL: https://www.youtube.com/watch?v=q3DLdMj5zLA\n",
        "\n",
        "I do not know if it is possible to install GDAL on WINDOWS inside a conda environment.\n",
        "\n",
        "On Ubuntu you have to follow these steps:\n",
        "\n",
        "\n",
        "\n",
        "Specific process for installation: https://stackoverflow.com/questions/44005694/no-module-named-gdal\n",
        "\n",
        "- sudo apt-get update && sudo apt upgrade -y && sudo apt autoremove \n",
        "- sudo apt-get install -y cdo nco gdal-bin libgdal-dev-\n",
        "- python -m pip install --upgrade pip setuptools wheel\n",
        "- python -m pip install --upgrade gdal\n",
        "- conda install -c conda forge libgdal\n",
        "- conda install -c conda-forge libgdal\n",
        "- conda install -c conda-forge gdal\n",
        "- conda install tiledb=2.2\n",
        "- conda install poppler\n",
        "\n",
        "When you have this you can hopefully vectorize the detected masks quite easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_fname_from_path(path):\n",
        "    \"\"\"\n",
        "    Given a path, returns the filename after the last frontslash character.\n",
        "    \"\"\"\n",
        "    return path.rsplit('/', 1)[-1]\n",
        "\n",
        "def get_fname_no_extension(path):\n",
        "    \"\"\"\n",
        "    Given a path, returns the filename without its extension.\n",
        "    \"\"\"\n",
        "    filename, extension = os.path.splitext(path)\n",
        "    return filename"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDWUhUkJaYl8"
      },
      "outputs": [],
      "source": [
        "import osgeo\n",
        "from osgeo import gdal\n",
        "from osgeo import ogr\n",
        "print('GDAL version: ', osgeo.gdal.__version__)\n",
        "\n",
        "# Choose which image to vectorize\n",
        "n  = 0\n",
        "\n",
        "input = regularizations[n]\n",
        "print()\n",
        "print(\"INPUT: \", input)\n",
        "\n",
        "# print(get_fname_no_extension(input))\n",
        "\n",
        "# out\n",
        "output = get_fname_from_path(get_fname_no_extension(input)) + \".gpkg\"\n",
        "print(\"OUTPUT: \", output)\n",
        "\n",
        "# Open image with GDAl driver\n",
        "ds = gdal.Open(input)\n",
        "# Get the band\n",
        "band = ds.GetRasterBand(1)\n",
        "\n",
        "# Create the output shapefile\n",
        "driver = ogr.GetDriverByName(\"GPKG\")\n",
        "out_ds = driver.CreateDataSource(output)\n",
        "out_layer = out_ds.CreateLayer(output, geom_type=ogr.wkbPolygon)\n",
        "\n",
        "# Add a field to the layer to store the pixel values\n",
        "field_defn = ogr.FieldDefn(\"Pix_Value\", ogr.OFTInteger)\n",
        "out_layer.CreateField(field_defn)\n",
        "\n",
        "# Polygonize the PNG file\n",
        "gdal.Polygonize(band, None, out_layer, 0, [], callback=None)\n",
        "\n",
        "# Close the input and output files\n",
        "out_ds = None\n",
        "ds = None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For the builing detection case we need to only keep the vectors with pixel value 255. Easiest solution is to use: Extract by attribute. The Python solution with GDAL can be found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ogr2ogr -where ID=\"1\" OUTFILE.gpkg INFILE.gpkg\n",
        "\n",
        "# RUN from the command line inside Ubuntu\n",
        "# Change name of input and output according to user needs\n",
        "\n",
        "!ogr2ogr -where Pix_Value=\"255\" bergen_-5943_1104B.gpkg bergen_-5943_1104.gpkg"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
