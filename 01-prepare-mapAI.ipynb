{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77a99cae-ee45-41ab-8a32-e57a7e0957b5",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook loads the MapAI data from the Huggingface. It also computes some basic information about the images: <br>\n",
    "* what percentage of each image is labeled as 1 in the mask, and \n",
    "* which images contain buildings and which are empty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a66fc06-78a2-4141-84f8-69fcc1cd8bfc",
   "metadata": {},
   "source": [
    "# Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140cf9ce-e9b2-4d4a-af9a-b1e0f75d280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shymon/anaconda3/envs/torch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# !pip install datasets\n",
    "\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "938e53a9-b0cf-4eb4-9bc6-19c7ba2944fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/shymon/.cache/huggingface/datasets/sjyhne___parquet/sjyhne--mapai_dataset-a29285241f23b7c3/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "100%|██████████| 4/4 [00:00<00:00, 72.36it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"sjyhne/mapai_dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c9bcad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['image', 'lidar', 'mask', 'filename'],\n",
       "        num_rows: 7000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['image', 'lidar', 'mask', 'filename'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "    task1_test: Dataset({\n",
       "        features: ['image', 'lidar', 'mask', 'filename'],\n",
       "        num_rows: 1368\n",
       "    })\n",
       "    task2_test: Dataset({\n",
       "        features: ['image', 'lidar', 'mask', 'filename'],\n",
       "        num_rows: 978\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650ff388-f69a-466d-89b9-4170919762ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shymon/datasets/mapai-full\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "#### Paths ####\n",
    "# Update this to wherever you want to store data\n",
    "DATADIR = Path(\"/home/shymon/datasets/mapai-full\")\n",
    "\n",
    "# MapAI\n",
    "MAPAI_TRAIN = DATADIR/'train'\n",
    "MAPAI_MASKS = MAPAI_TRAIN/'masks'\n",
    "MAPAI_PATCHES = MAPAI_TRAIN/'patches'\n",
    "\n",
    "MAPAI_VAL = DATADIR/'validation'\n",
    "MAPAI_VAL_MASKS = MAPAI_VAL/'masks'\n",
    "MAPAI_VAL_PREDS = MAPAI_VAL/'predictions'\n",
    "\n",
    "print(DATADIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa19e3e8-90ed-4a7d-b58d-fc33cc49e843",
   "metadata": {},
   "source": [
    "Move the data to a user-defined location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad59610e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/shutil.py:816\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    815\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m     os\u001b[39m.\u001b[39;49mrename(src, real_dst)\n\u001b[1;32m    817\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mOSError\u001b[39;00m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../datasets' -> '/home/shymon/datasets/mapai-full/datasets'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m move \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mif\u001b[39;00m move:\n\u001b[0;32m----> 3\u001b[0m     shutil\u001b[39m.\u001b[39;49mmove(\u001b[39m\"\u001b[39;49m\u001b[39m../../datasets\u001b[39;49m\u001b[39m\"\u001b[39;49m, DATADIR)\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/shutil.py:836\u001b[0m, in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    834\u001b[0m         rmtree(src)\n\u001b[1;32m    835\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 836\u001b[0m         copy_function(src, real_dst)\n\u001b[1;32m    837\u001b[0m         os\u001b[39m.\u001b[39munlink(src)\n\u001b[1;32m    838\u001b[0m \u001b[39mreturn\u001b[39;00m real_dst\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/shutil.py:434\u001b[0m, in \u001b[0;36mcopy2\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misdir(dst):\n\u001b[1;32m    433\u001b[0m     dst \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dst, os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(src))\n\u001b[0;32m--> 434\u001b[0m copyfile(src, dst, follow_symlinks\u001b[39m=\u001b[39;49mfollow_symlinks)\n\u001b[1;32m    435\u001b[0m copystat(src, dst, follow_symlinks\u001b[39m=\u001b[39mfollow_symlinks)\n\u001b[1;32m    436\u001b[0m \u001b[39mreturn\u001b[39;00m dst\n",
      "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.10/shutil.py:254\u001b[0m, in \u001b[0;36mcopyfile\u001b[0;34m(src, dst, follow_symlinks)\u001b[0m\n\u001b[1;32m    252\u001b[0m     os\u001b[39m.\u001b[39msymlink(os\u001b[39m.\u001b[39mreadlink(src), dst)\n\u001b[1;32m    253\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(src, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fsrc:\n\u001b[1;32m    255\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(dst, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m fdst:\n\u001b[1;32m    257\u001b[0m                 \u001b[39m# macOS\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../datasets'"
     ]
    }
   ],
   "source": [
    "move = True\n",
    "if move:\n",
    "    shutil.move(\"../../../data\", DATADIR) # How the fuck does this work? # Never run with ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a7b97-4d16-455c-a0c0-b92ba39f19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val = dataset['train'], dataset['validation']\n",
    "test1, test2 = dataset['task1_test'], dataset['task2_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592c1476-18f1-44b0-9958-8179da8c004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataset: ' '\\n', train)\n",
    "print('-----------------------------------------')\n",
    "print('Validation dataset: ', '\\n', val)\n",
    "\n",
    "print('Test 1: ' '\\n', test1)\n",
    "print('-----------------------------------------')\n",
    "print('Test 2: ', '\\n', test2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aca8e591",
   "metadata": {},
   "source": [
    "Get one instance from the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1fbb5181ef0f8b108572537263a3b8da81dbe66f2ac86d8e5730a4770eab3e79"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
